python -m Pretraining.preprocess --input_dir C:/Users/pauld/Projects/IOA/2_2_ProgrammTransfer/dataset --output_dir ./kqa_processed

python -m Pretraining.preprocess --input_dir ./kqa_dataset_ent --output_dir ./kqa_processed_ent


python -m Pretraining.pushhub --save_dir ./train/checkpoint-11999 --name IOA_261022

python -m Pretraining.pushhub --save_dir ./train/checkpoint-11999 --name IOA_261022_test --input_dir ./processed/entity


python -m  Preprocessing_KG.transform_graphdb

python -m Pretraining.preprocess_ESA --train_file_path ./test_data/IOA_test.json --output_dir ./test_data --input_dir ./test_data

python -m Pretraining.eval --models_dir ./train_ioa_kep_concat --data_dir ./processed_ioa/

python -m Pretraining.eval --models_dir ./train_pret_cosmic --data_dir ./processed_ioa_rob/

python -m Pretraining.eval --model_dir ./train --data_dir 

python -m Pretraining.train --input_dir ./kqa_processed_ent --output_dir ./train_ent --save_dir ./train_ent --model_name_or_path bert-base-uncased --val_batch_size=256 --train_batch_size=128 --learning_rate=1e-5 --save_steps=2 --logging_steps=1 --num_train_epochs=100 --wandb=1

python -m Pretraining.train_rob --input_dir ./kqa_processed_ent_rob --output_dir ./train_pret_cosmicrob --save_dir ./train_pret_cosmicrob --model_name_or_path icelab/cosmicroberta --val_batch_size=256 --train_batch_size=128 --learning_rate=1e-5 --save_steps=2400 --logging_steps=1200 --num_train_epochs=100 --wandb=1 --device cuda:1

python -m Pretraining.train_rob --input_dir ./processed_ioa_rob --output_dir ./train_pret_kep_2 --save_dir ./train_pret_kep_2 --model_name_or_path ./train_ent_rob_kep/checkpoint-59999 --val_batch_size=256 --train_batch_size=128 --learning_rate=1e-5 --save_steps=120 --logging_steps=2 --eval_steps=30 --num_train_epochs=40 --device cuda:0 --wandb=1

python -m Pretraining.train_rob --input_dir ./processed_ioa_rob --output_dir ./train_kepler_concat --save_dir ./train_kepler_concat --model_name_or_path ./kepler_models/hf/esa_ioa_concat --val_batch_size=256 --train_batch_size=128 --learning_rate=1e-5 --save_steps=120 --logging_steps=2 --eval_steps=30 --num_train_epochs=80 --device cuda:0 --wandb=1

python -m Pretraining.train_rob --input_dir ./processed_ioa_rob --output_dir ./train_roberta --save_dir ./train_roberta --model_name_or_path roberta-base --val_batch_size=256 --train_batch_size=128 --learning_rate=1e-5 --save_steps=120 --logging_steps=2 --eval_steps=30 --num_train_epochs=80 --device cuda:0 --wandb=1



python -m Pretraining.train_rob --input_dir ./processed_ioa_bert --output_dir ./train_pret_bert --save_dir ./train_pret_bert --model_name_or_path ./train_ent_ioa_base/checkpoint-45599 --val_batch_size=256 --train_batch_size=128 --learning_rate=1e-5 --save_steps=120 --logging_steps=2 --eval_steps=2 --num_train_epochs=40 --device cuda:0 --wandb=1

python -m Pretraining.preprocess_ESA --input_dir ./Preprocessing_KG --train_file_path ./processed_ioa_bert/ioa_train_1001.json --valid_file_path ./processed_ioa_bert/ioa_valid_1001.json  --output_dir ./processed_ioa_bert                   


python -m Pretraining.train_rob --input_dir ./processed_ioa_rob --output_dir ./train_ioa_cosmic_ --save_dir ./train_ioa_cosmic_ --model_name_or_path ./train_pret_cosmicrob/checkpoint-146399 --val_batch_size=256 --train_batch_size=128 --learning_rate=1e-5 --save_steps=120 --logging_steps=2 --eval_steps=2 --num_train_epochs=40 --device cuda:0 --wandb=1



## Evaluation
python -m Pretraining.train_eval --input_dir ./test_data --output_dir ./evaluate --save_dir ./evaluate --model_name_or_path PaulD/IOA_261022-11999 --val_batch_size=8

python -m Pretraining.train --input_dir ./test_data --output_dir ./fine_tuning --save_dir ./fine_tuning --model_name_or_path PaulD/IOA_261022-11999 --val_batch_size=32 --train_batch_size=32 --learning_rate=1e-5 --save_steps==2 --logging_steps==1 --num_train_epochs=10


python -m Pretraining.preprocess_ESA --input_dir ./Preprocessing_KG --train_file_path ./test_data/IOA_test.json --output_dir ./test_data

python -m Pretraining.preprocess_ESA --input_dir ./Preprocessing_KG --train_file_path ./test_data/IOA_test.json --output_dir ./test_data

https://maven.apache.org/install.html

python -m transformers.convert_roberta_original_pytorch_checkpoint_to_pytorch \
			--roberta_checkpoint_path C:/Users/pauld/Projects/IOA/3_10_Keplermodels/esa_ioa_kepler_checkpoints/esa_ioa_concat_checkpoint_best.pt path_to_KEPLER_checkpoint \
			--pytorch_dump_folder_path ./Kepler \


python -m Pretraining.transform_Keplercp --roberta_checkpoint_path C:/Users/pauld/Projects/IOA/3_10_Keplermodels/esa_ioa_kepler_checkpoints/esa_ioa_concat --pytorch_dump_folder_path ./Kepler 

python -m Pretraining.transform_Keplercp --roberta_checkpoint_path C:/Users/pauld/Projects/IOA/3_10_Keplermodels/esa_ioa_kepler_checkpoints/esa_ioa_concat_checkpoint_best.pt --pytorch_dump_folder_path ./Kepler 